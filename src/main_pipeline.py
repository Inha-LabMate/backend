"""
í†µí•© í¬ë¡¤ë§ & ì„ë² ë”© íŒŒì´í”„ë¼ì¸
==============================

ì´ íŒŒì¼ì€ ì „ì²´ ì‹œìŠ¤í…œì„ í†µí•©í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.

ì „ì²´ íë¦„:
    1. ì›¹í˜ì´ì§€ í¬ë¡¤ë§ (requests + BeautifulSoup)
       â†“
    2. HTMLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ (chunking.py)
       â†“
    3. í…ìŠ¤íŠ¸ë¥¼ 200-400ì ë‹¨ìœ„ë¡œ ë¶„í•  (chunking.py)
       â†“
    4. í…ìŠ¤íŠ¸ ì •ê·œí™” (text_normalization.py)
       - ì–¸ì–´ ê°ì§€
       - ì—°ë½ì²˜ ì¶”ì¶œ
       - í´ë¦°ì—…
       â†“
    5. ì„ë² ë”© ìƒì„± (embedding.py)
       - í…ìŠ¤íŠ¸ â†’ 768ì°¨ì› ë²¡í„° ë³€í™˜
       â†“
    6. ì €ì¥ (local_storage.py ë˜ëŠ” vector_db.py)
       - ë¡œì»¬ JSON íŒŒì¼ ë˜ëŠ” PostgreSQL

ì‹¤í–‰ ë°©ë²•:
    python main_pipeline.py

ì„¤ì •:
    - USE_LOCAL = True  â†’ ë¡œì»¬ JSON íŒŒì¼ ì €ì¥
    - USE_LOCAL = False â†’ PostgreSQL ì €ì¥
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from typing import List, Dict, Optional
from datetime import datetime
import traceback

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from chunking import DocumentProcessor, Chunk
from text_normalization import TextNormalizer
from embedding import EmbeddingPipeline

# ============================================================================
# ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ì„¤ì •
# USE_LOCAL = True  â†’ ë¡œì»¬ JSON íŒŒì¼ ì €ì¥ (PostgreSQL ë¶ˆí•„ìš”)
# USE_LOCAL = False â†’ PostgreSQL + pgvector ì‚¬ìš©
# ============================================================================
USE_LOCAL = True  # â† ì´ ê°’ì„ Falseë¡œ ë°”ê¾¸ë©´ PostgreSQL ì‚¬ìš©

if USE_LOCAL:
    # ë¡œì»¬ íŒŒì¼ ì €ì¥ì†Œ ì‚¬ìš©
    from local_storage import LocalVectorStore as VectorDatabase
    print("âœ… ë¡œì»¬ íŒŒì¼ ì €ì¥ì†Œ ëª¨ë“œ")
else:
    # PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©
    from vector_db import VectorDatabase, DatabaseConfig, LabDocument
    print("âœ… PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“œ")


class CrawlConfig:
    """
    í¬ë¡¤ë§ ì„¤ì • í´ë˜ìŠ¤
    
    í¬ë¡¤ë§ ë™ì‘ì„ ì œì–´í•˜ëŠ” ì„¤ì •ê°’ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.
    
    ì†ì„±:
        MAX_PAGES (int): ì—°êµ¬ì‹¤ë‹¹ ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜
            ì˜ˆ) 5 â†’ ë©”ì¸ í˜ì´ì§€ + ë§í¬ëœ í˜ì´ì§€ 4ê°œ
        TIMEOUT (int): HTTP ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        DELAY (int): í˜ì´ì§€ ê°„ ë”œë ˆì´ (ì´ˆ)
            â†’ ì„œë²„ ë¶€ë‹´ì„ ì¤„ì´ê¸° ìœ„í•œ ëŒ€ê¸° ì‹œê°„
        USER_AGENT (str): ë¸Œë¼ìš°ì € ì‹ë³„ ë¬¸ìì—´
            â†’ ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” User-Agent í™•ì¸
        MIN_TEXT_LENGTH (int): ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ (ë¬¸ì)
            â†’ ì´ë³´ë‹¤ ì§§ì€ ì²­í¬ëŠ” ë²„ë¦¼
        MIN_QUALITY_SCORE (int): ìµœì†Œ í’ˆì§ˆ ì ìˆ˜
            â†’ ì´ë³´ë‹¤ ë‚®ì€ ì²­í¬ëŠ” ë²„ë¦¼
    
    ì‚¬ìš© ì˜ˆ:
        config = CrawlConfig()
        config.MAX_PAGES = 10  # ë” ë§ì€ í˜ì´ì§€ í¬ë¡¤ë§
    """
    MAX_PAGES = 5  # ì—°êµ¬ì‹¤ë‹¹ ìµœëŒ€ 5í˜ì´ì§€
    TIMEOUT = 10   # 10ì´ˆ íƒ€ì„ì•„ì›ƒ
    DELAY = 1      # í˜ì´ì§€ ê°„ 1ì´ˆ ëŒ€ê¸°
    USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    
    # í’ˆì§ˆ ê¸°ì¤€
    MIN_TEXT_LENGTH = 100    # ìµœì†Œ 100ì
    MIN_QUALITY_SCORE = 30   # ìµœì†Œ í’ˆì§ˆ ì ìˆ˜ 30ì 


class LabCrawler:
    """
    ì—°êµ¬ì‹¤ í¬ë¡¤ëŸ¬
    
    í•˜ë‚˜ì˜ ì—°êµ¬ì‹¤ ì›¹ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    ì²˜ë¦¬ ê³¼ì •:
        1. ì—°êµ¬ì‹¤ ê¸°ë³¸ ì •ë³´ DB ì €ì¥
        2. í™ˆí˜ì´ì§€ í¬ë¡¤ë§
        3. ê´€ë ¨ í˜ì´ì§€ ë°œê²¬ (research, publication ë“±)
        4. ê° í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ
        5. ì²­í‚¹ (200-400ì ë‹¨ìœ„ë¡œ ë¶„í• )
        6. ì •ê·œí™” (ì–¸ì–´ ê°ì§€, í´ë¦°ì—…)
        7. ì„ë² ë”© (í…ìŠ¤íŠ¸ â†’ ë²¡í„°)
        8. DB ì €ì¥
    
    ì£¼ìš” ë©”ì„œë“œ:
        crawl_lab()       - ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        _discover_pages() - ê´€ë ¨ í˜ì´ì§€ ì°¾ê¸°
        _crawl_page()     - ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§
        _process_chunk()  - ì²­í¬ ì²˜ë¦¬ (ì •ê·œí™” + ì„ë² ë”©)
    
    ì‚¬ìš© ì˜ˆ:
        crawler = LabCrawler(db, embedding_pipeline)
        result = crawler.crawl_lab(lab_data)
        print(f"ì €ì¥ëœ ì²­í¬: {result['chunks_saved']}ê°œ")
    """
    
    def __init__(
        self,
        db: VectorDatabase,
        embedding_pipeline: EmbeddingPipeline,
        config: CrawlConfig = CrawlConfig()
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            db: ë°ì´í„°ë² ì´ìŠ¤ ê°ì²´ (ë¡œì»¬ ë˜ëŠ” PostgreSQL)
            embedding_pipeline: ì„ë² ë”© íŒŒì´í”„ë¼ì¸
            config: í¬ë¡¤ë§ ì„¤ì •
        """
        self.db = db
        self.embedding_pipeline = embedding_pipeline
        self.config = config
        
        # í•˜ìœ„ ëª¨ë“ˆ ì´ˆê¸°í™”
        self.doc_processor = DocumentProcessor()      # HTML â†’ ì²­í¬
        self.text_normalizer = TextNormalizer()       # í…ìŠ¤íŠ¸ ì •ê·œí™”
        
        self.visited_urls = set()  # ì¤‘ë³µ ë°©ë¬¸ ë°©ì§€
    
    def crawl_lab(self, lab_data: Dict) -> Dict:
        """
        ë‹¨ì¼ ì—°êµ¬ì‹¤ í¬ë¡¤ë§
        
        Args:
            lab_data: ì—°êµ¬ì‹¤ ê¸°ë³¸ ì •ë³´
        
        Returns:
            í¬ë¡¤ë§ ê²°ê³¼ í†µê³„
        """
        start_time = time.time()
        
        result = {
            'lab_id': None,
            'success': False,
            'pages_visited': 0,
            'chunks_created': 0,
            'chunks_saved': 0,
            'error': None
        }
        
        try:
            # 1. ì—°êµ¬ì‹¤ ì •ë³´ DBì— ì €ì¥
            lab_id = self.db.insert_lab({
                'kor_name': lab_data.get('ì—°êµ¬ì‹¤ëª…(í•œê¸€)', ''),
                'eng_name': lab_data.get('ì—°êµ¬ì‹¤ëª…(ì˜ë¬¸)', ''),
                'professor': lab_data.get('ì§€ë„êµìˆ˜', ''),
                'homepage': lab_data.get('ì›¹ì‚¬ì´íŠ¸', ''),
                'location': lab_data.get('ì—°êµ¬ì‹¤ìœ„ì¹˜', ''),
                'contact_email': lab_data.get('ì´ë©”ì¼', ''),
                'contact_phone': lab_data.get('ì—°ë½ì²˜', ''),
                'description': lab_data.get('ì—°êµ¬ë‚´ìš©', '')
            })
            
            result['lab_id'] = lab_id
            
            # 2. ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§
            homepage = lab_data.get('ì›¹ì‚¬ì´íŠ¸', '')
            if not homepage or homepage == 'í•´ë‹¹ì—†ìŒ':
                result['error'] = 'NO_WEBSITE'
                self.db.log_crawl(
                    lab_id=lab_id,
                    url='',
                    status='NO_WEBSITE',
                    duration=time.time() - start_time
                )
                return result
            
            # 3. í˜ì´ì§€ í¬ë¡¤ë§ & ì²­í‚¹
            all_chunks = []
            pages = self._discover_pages(homepage)
            
            for i, page_url in enumerate(pages[:self.config.MAX_PAGES]):
                if page_url in self.visited_urls:
                    continue
                
                try:
                    chunks = self._crawl_page(page_url, lab_id, crawl_depth=i)
                    all_chunks.extend(chunks)
                    result['pages_visited'] += 1
                    
                    time.sleep(self.config.DELAY)
                except Exception as e:
                    print(f"    âš ï¸  í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {page_url} - {e}")
            
            result['chunks_created'] = len(all_chunks)
            
            # 4. í…ìŠ¤íŠ¸ ì •ê·œí™” & ì„ë² ë”©
            documents = []
            for chunk in all_chunks:
                try:
                    doc_data = self._process_chunk(chunk, lab_id)
                    if doc_data:
                        documents.append(doc_data)
                except Exception as e:
                    print(f"    âš ï¸  ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            # 5. DB ì €ì¥
            if USE_LOCAL:
                # ë¡œì»¬ ì €ì¥ì†Œìš©
                saved_ids = self.db.insert_documents_batch(lab_id, documents)
            else:
                # PostgreSQLìš© (ì£¼ì„ì²˜ë¦¬ - ë‚˜ì¤‘ì— ë³µì› ê°€ëŠ¥)
                # saved_ids = self.db.insert_documents_batch(documents)
                saved_ids = []
            
            result['chunks_saved'] = len(saved_ids)
            
            # 6. í¬ë¡¤ë§ ìƒíƒœ ì—…ë°ì´íŠ¸
            status = 'SUCCESS' if result['chunks_saved'] > 0 else 'NO_CONTENT'
            self.db.update_lab_crawl_status(
                lab_id=lab_id,
                status=status,
                quality_score=self._calculate_quality_score(result)
            )
            
            # 7. ë¡œê·¸ ê¸°ë¡
            self.db.log_crawl(
                lab_id=lab_id,
                url=homepage,
                status=status,
                pages_visited=result['pages_visited'],
                chunks_created=result['chunks_saved'],
                duration=time.time() - start_time
            )
            
            result['success'] = True
            
        except Exception as e:
            result['error'] = str(e)
            result['traceback'] = traceback.format_exc()
            
            if result['lab_id']:
                self.db.log_crawl(
                    lab_id=result['lab_id'],
                    url=lab_data.get('ì›¹ì‚¬ì´íŠ¸', ''),
                    status='FAILED',
                    duration=time.time() - start_time,
                    error_message=str(e)
                )
        
        return result
    
    def _discover_pages(self, base_url: str) -> List[str]:
        """ê´€ë ¨ í˜ì´ì§€ ë°œê²¬"""
        pages = [base_url]
        
        try:
            response = requests.get(
                base_url,
                timeout=self.config.TIMEOUT,
                headers={'User-Agent': self.config.USER_AGENT},
                allow_redirects=True  # ë¦¬ë‹¤ì´ë ‰íŠ¸ í—ˆìš©
            )
            response.raise_for_status()
            
            # ì‹¤ì œ URL (ë¦¬ë‹¤ì´ë ‰íŠ¸ í›„)
            actual_url = response.url
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ê´€ë ¨ ë§í¬ ì°¾ê¸°
            relevant_keywords = [
                'research', 'publication', 'people', 'member', 'about',
                'project', 'lab', 'ì—°êµ¬', 'ë…¼ë¬¸', 'êµ¬ì„±ì›', 'ì†Œê°œ'
            ]
            
            for a_tag in soup.find_all('a', href=True):
                href = a_tag.get('href', '')
                text = a_tag.get_text(strip=True).lower()
                
                if any(kw in href.lower() or kw in text for kw in relevant_keywords):
                    # ì ˆëŒ€ URL ë³€í™˜
                    from urllib.parse import urljoin, urlparse
                    full_url = urljoin(actual_url, href)  # ë¦¬ë‹¤ì´ë ‰íŠ¸ëœ URL ì‚¬ìš©
                    
                    # ê°™ì€ ë„ë©”ì¸ë§Œ
                    if urlparse(full_url).netloc == urlparse(actual_url).netloc:
                        # ì¤‘ë³µ ê²½ë¡œ í™•ì¸ (ì˜ˆ: /view/vcl-lab/view/vcl-lab)
                        parsed = urlparse(full_url)
                        path = parsed.path
                        
                        # ê²½ë¡œ ì¤‘ë³µ ì œê±° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                        path_parts = [p for p in path.split('/') if p]
                        seen = set()
                        unique_parts = []
                        for part in path_parts:
                            if part not in seen or part in ['view', 'page']:  # 'view', 'page'ëŠ” ì¤‘ë³µ í—ˆìš©
                                unique_parts.append(part)
                                seen.add(part)
                        
                        cleaned_path = '/' + '/'.join(unique_parts)
                        cleaned_url = f"{parsed.scheme}://{parsed.netloc}{cleaned_path}"
                        
                        if cleaned_url not in pages and cleaned_url != base_url:
                            pages.append(cleaned_url)
        
        except Exception as e:
            print(f"    âš ï¸  í˜ì´ì§€ ë°œê²¬ ì‹¤íŒ¨: {e}")
        
        return pages
    
    def _crawl_page(self, url: str, lab_id: int, crawl_depth: int) -> List[Chunk]:
        """ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§"""
        self.visited_urls.add(url)
        
        try:
            # HTML ê°€ì ¸ì˜¤ê¸°
            response = requests.get(
                url,
                timeout=self.config.TIMEOUT,
                headers={'User-Agent': self.config.USER_AGENT},
                allow_redirects=True
            )
            response.raise_for_status()
            
            # ì‘ë‹µ ë‚´ìš© í™•ì¸
            if not response.text or len(response.text) < 100:
                print(f"    âš ï¸  ë¹ˆ ì‘ë‹µ: {url}")
                return []
            
            # ì²­í‚¹
            chunks = self.doc_processor.process_html(
                html=response.text,
                url=url,
                crawl_depth=crawl_depth
            )
            
            return chunks
            
        except requests.exceptions.HTTPError as e:
            # HTTP ì—ëŸ¬ (404, 403 ë“±)ëŠ” ì¡°ìš©íˆ ì²˜ë¦¬
            if e.response.status_code in [404, 403, 410]:
                print(f"    âš ï¸  í˜ì´ì§€ ì—†ìŒ ({e.response.status_code}): {url}")
            else:
                print(f"    âš ï¸  HTTP ì—ëŸ¬ ({e.response.status_code}): {url}")
            return []
            
        except requests.exceptions.RequestException as e:
            # ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ (íƒ€ì„ì•„ì›ƒ, ì—°ê²° ì‹¤íŒ¨ ë“±)
            print(f"    âš ï¸  ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬: {url} - {str(e)}")
            return []
            
        except Exception as e:
            # ê¸°íƒ€ ì—ëŸ¬ (íŒŒì‹± ì—ëŸ¬ ë“±)
            print(f"    âš ï¸  ì²˜ë¦¬ ì—ëŸ¬: {url} - {type(e).__name__}: {str(e)}")
            return []
    
    def _process_chunk(self, chunk: Chunk, lab_id: int) -> Optional[Dict]:
        """ì²­í¬ ì²˜ë¦¬ (ì •ê·œí™” + ì„ë² ë”©)"""
        # 1. í…ìŠ¤íŠ¸ ì •ê·œí™”
        normalized = self.text_normalizer.normalize(chunk.text)
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
        if len(normalized.cleaned_text) < self.config.MIN_TEXT_LENGTH:
            return None
        
        # 2. ì„ë² ë”© ìƒì„±
        emb_result = self.embedding_pipeline.embed(normalized.cleaned_text)
        
        # 3. ë¬¸ì„œ ë°ì´í„° ìƒì„± (Dict í˜•íƒœ)
        doc_data = {
            'section': chunk.section,
            'title': chunk.title,
            'text': normalized.cleaned_text,
            'lang': normalized.language,
            'tokens': normalized.tokens,
            'source_url': chunk.source_url,
            'parent_url': chunk.source_url,
            'crawl_depth': chunk.crawl_depth,
            'source_type': 'html',
            'md5': chunk.md5,
            'embedding': emb_result.embedding.tolist() if USE_LOCAL else emb_result.embedding,
            'emb_model': emb_result.model_name,
            'emb_ver': emb_result.model_version,
            'quality_score': self._calculate_chunk_quality(chunk, normalized)
        }
        
        return doc_data
    
    # ========================================================================
    # PostgreSQLìš© ê¸°ì¡´ ì½”ë“œ (ì£¼ì„ì²˜ë¦¬ - ë‚˜ì¤‘ì— ë³µì› ê°€ëŠ¥)
    # ========================================================================
    # def _process_chunk(self, chunk: Chunk, lab_id: int) -> Optional[LabDocument]:
    #     """ì²­í¬ ì²˜ë¦¬ (ì •ê·œí™” + ì„ë² ë”©)"""
    #     # 1. í…ìŠ¤íŠ¸ ì •ê·œí™”
    #     normalized = self.text_normalizer.normalize(chunk.text)
    #     
    #     # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ìŠ¤í‚µ
    #     if len(normalized.cleaned_text) < self.config.MIN_TEXT_LENGTH:
    #         return None
    #     
    #     # 2. ì„ë² ë”© ìƒì„±
    #     emb_result = self.embedding_pipeline.embed(normalized.cleaned_text)
    #     
    #     # 3. LabDocument ìƒì„±
    #     doc = LabDocument(
    #         lab_id=lab_id,
    #         section=chunk.section,
    #         title=chunk.title,
    #         text=normalized.cleaned_text,
    #         lang=normalized.language,
    #         tokens=normalized.tokens,
    #         source_url=chunk.source_url,
    #         parent_url=chunk.source_url,  # ì¼ë‹¨ ë™ì¼í•˜ê²Œ
    #         crawl_depth=chunk.crawl_depth,
    #         source_type='html',
    #         md5=chunk.md5,
    #         embedding=emb_result.embedding,
    #         emb_model=emb_result.model_name,
    #         emb_ver=emb_result.model_version,
    #         quality_score=self._calculate_chunk_quality(chunk, normalized)
    #     )
    #     
    #     return doc
    
    def _calculate_chunk_quality(self, chunk: Chunk, normalized) -> int:
        """ì²­í¬ í’ˆì§ˆ ì ìˆ˜ (0-100)"""
        score = 50  # ê¸°ë³¸ ì ìˆ˜
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´
        if len(normalized.cleaned_text) > 500:
            score += 20
        elif len(normalized.cleaned_text) > 300:
            score += 10
        
        # ì–¸ì–´ ëª…í™•ì„±
        if normalized.language in ['ko', 'en']:
            score += 15
        
        # í† í° ìˆ˜
        if normalized.tokens > 100:
            score += 10
        
        # ì œëª© ì¡´ì¬
        if chunk.title:
            score += 5
        
        return min(score, 100)
    
    def _calculate_quality_score(self, result: Dict) -> int:
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜"""
        score = 0
        
        if result['pages_visited'] > 0:
            score += 30
        
        if result['chunks_saved'] >= 5:
            score += 40
        elif result['chunks_saved'] >= 2:
            score += 20
        
        if result['success']:
            score += 30
        
        return min(score, 100)


class CrawlOrchestrator:
    """í¬ë¡¤ë§ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    
    def __init__(
        self,
        db_config=None,  # Noneì´ë©´ ë¡œì»¬ ì €ì¥ì†Œ ì‚¬ìš©
        embedding_model: str = 'multilingual-mpnet',
        device: str = 'cpu',
        local_data_dir: str = './crawl_data'  # ë¡œì»¬ ì €ì¥ì†Œ ê²½ë¡œ
    ):
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        if USE_LOCAL:
            # ë¡œì»¬ ì €ì¥ì†Œ ì‚¬ìš©
            self.db = VectorDatabase(data_dir=local_data_dir)
            print(f"âœ… ë¡œì»¬ ì €ì¥ì†Œ ê²½ë¡œ: {local_data_dir}")
        else:
            # PostgreSQL ì‚¬ìš© (ì£¼ì„ì²˜ë¦¬ - ë‚˜ì¤‘ì— ë³µì› ê°€ëŠ¥)
            # if db_config is None:
            #     raise ValueError("PostgreSQL ëª¨ë“œì—ì„œëŠ” db_configê°€ í•„ìš”í•©ë‹ˆë‹¤")
            # self.db = VectorDatabase(db_config)
            pass
        
        # ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        print("ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”...")
        self.embedding_pipeline = EmbeddingPipeline(
            model_name=embedding_model,
            device=device,
            use_cache=True
        )
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {embedding_model}\n")
    
    def crawl_from_url(self, url: str) -> pd.DataFrame:
        """
        URLì—ì„œ ì—°êµ¬ì‹¤ ëª©ë¡ í¬ë¡¤ë§ í›„ ê° ì—°êµ¬ì‹¤ ì²˜ë¦¬
        
        Args:
            url: ì—°êµ¬ì‹¤ ëª©ë¡ í˜ì´ì§€ URL
        """
        print("="*80)
        print("1ë‹¨ê³„: ì—°êµ¬ì‹¤ ëª©ë¡ í¬ë¡¤ë§")
        print("="*80)
        
        # ì—°êµ¬ì‹¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        response = requests.get(url)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        all_labs = soup.find_all('div', class_='labs')
        
        labs_data = []
        
        for lab_div in all_labs:
            dl = lab_div.find('dl')
            if not dl:
                continue
            
            lab_info = {
                'ì—°êµ¬ì‹¤ëª…(í•œê¸€)': '',
                'ì—°êµ¬ì‹¤ëª…(ì˜ë¬¸)': '',
                'ì§€ë„êµìˆ˜': '',
                'ì—°êµ¬ë‚´ìš©': '',
                'ì—°êµ¬ì‹¤ìœ„ì¹˜': '',
                'ì—°ë½ì²˜': '',
                'ì´ë©”ì¼': '',
                'ì›¹ì‚¬ì´íŠ¸': ''
            }
            
            dt = dl.find('dt')
            if dt:
                dt_text = dt.get_text(strip=True)
                small = dt.find('small')
                
                if small:
                    lab_info['ì—°êµ¬ì‹¤ëª…(ì˜ë¬¸)'] = small.get_text(strip=True)
                    lab_info['ì—°êµ¬ì‹¤ëª…(í•œê¸€)'] = dt_text.replace(
                        lab_info['ì—°êµ¬ì‹¤ëª…(ì˜ë¬¸)'], ''
                    ).strip()
                else:
                    lab_info['ì—°êµ¬ì‹¤ëª…(í•œê¸€)'] = dt_text
            
            dds = dl.find_all('dd')
            for dd in dds:
                span = dd.find('span')
                if span:
                    field_name = span.get_text(strip=True)
                    value_text = dd.get_text(strip=True).replace(field_name, '', 1).strip()
                    
                    a_tag = dd.find('a')
                    if a_tag and a_tag.get('href'):
                        value_text = a_tag.get('href')
                    
                    if 'ì§€ë„êµìˆ˜' in field_name:
                        lab_info['ì§€ë„êµìˆ˜'] = value_text
                    elif 'ì—°êµ¬ë‚´ìš©' in field_name:
                        lab_info['ì—°êµ¬ë‚´ìš©'] = value_text
                    elif 'ì—°êµ¬ì‹¤' in field_name:
                        lab_info['ì—°êµ¬ì‹¤ìœ„ì¹˜'] = value_text
                    elif 'ì—°ë½ì²˜' in field_name:
                        lab_info['ì—°ë½ì²˜'] = value_text
                    elif 'ì´ë©”ì¼' in field_name:
                        lab_info['ì´ë©”ì¼'] = value_text
                    elif 'ì›¹ì‚¬ì´íŠ¸' in field_name:
                        lab_info['ì›¹ì‚¬ì´íŠ¸'] = value_text
            
            labs_data.append(lab_info)
        
        df = pd.DataFrame(labs_data)
        print(f"âœ… {len(df)}ê°œ ì—°êµ¬ì‹¤ ë°œê²¬\n")
        
        # ê° ì—°êµ¬ì‹¤ í¬ë¡¤ë§
        return self.crawl_labs(df)
    
    def crawl_labs(self, labs_df: pd.DataFrame) -> pd.DataFrame:
        """
        ì—°êµ¬ì‹¤ ë°ì´í„°í”„ë ˆì„ í¬ë¡¤ë§
        """
        print("="*80)
        print("2ë‹¨ê³„: ê° ì—°êµ¬ì‹¤ ìƒì„¸ í¬ë¡¤ë§ & ì„ë² ë”©")
        print("="*80)
        
        # ê²°ê³¼ ì»¬ëŸ¼ ì¶”ê°€
        labs_df['lab_id'] = None
        labs_df['pages_visited'] = 0
        labs_df['chunks_created'] = 0
        labs_df['chunks_saved'] = 0
        labs_df['crawl_status'] = ''
        labs_df['quality_score'] = 0
        labs_df['crawl_timestamp'] = ''
        labs_df['error'] = ''
        
        if USE_LOCAL:
            # ë¡œì»¬ ì €ì¥ì†Œ ì‚¬ìš© (with ë¬¸ ë¶ˆí•„ìš”)
            crawler = LabCrawler(self.db, self.embedding_pipeline)
            
            for idx, row in labs_df.iterrows():
                print(f"\n{'='*80}")
                print(f"[{idx+1}/{len(labs_df)}] {row['ì—°êµ¬ì‹¤ëª…(í•œê¸€)']}")
                print(f"{'='*80}")
                
                result = crawler.crawl_lab(row.to_dict())
                
                # ê²°ê³¼ ê¸°ë¡
                labs_df.at[idx, 'lab_id'] = result['lab_id']
                labs_df.at[idx, 'pages_visited'] = result['pages_visited']
                labs_df.at[idx, 'chunks_created'] = result['chunks_created']
                labs_df.at[idx, 'chunks_saved'] = result['chunks_saved']
                labs_df.at[idx, 'crawl_status'] = 'SUCCESS' if result['success'] else 'FAILED'
                labs_df.at[idx, 'crawl_timestamp'] = datetime.now().isoformat()
                
                if result['error']:
                    labs_df.at[idx, 'error'] = result['error']
                
                # ì¶œë ¥
                print(f"  ê²°ê³¼:")
                print(f"    - Lab ID: {result['lab_id']}")
                print(f"    - ë°©ë¬¸ í˜ì´ì§€: {result['pages_visited']}")
                print(f"    - ìƒì„± ì²­í¬: {result['chunks_created']}")
                print(f"    - ì €ì¥ ì²­í¬: {result['chunks_saved']}")
                print(f"    - ìƒíƒœ: {labs_df.at[idx, 'crawl_status']}")
                
                if result['error']:
                    print(f"    - ì˜¤ë¥˜: {result['error']}")
        
        else:
            # PostgreSQL ì‚¬ìš© (ì£¼ì„ì²˜ë¦¬ - ë‚˜ì¤‘ì— ë³µì› ê°€ëŠ¥)
            # with VectorDatabase(self.db_config) as db:
            #     crawler = LabCrawler(db, self.embedding_pipeline)
            #     
            #     for idx, row in labs_df.iterrows():
            #         ... (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
            pass
        
        return labs_df
    
    def print_summary(self, df: pd.DataFrame):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        print(f"  ì´ ì—°êµ¬ì‹¤: {len(df)}")
        print(f"  ì„±ê³µ: {(df['crawl_status'] == 'SUCCESS').sum()}")
        print(f"  ì‹¤íŒ¨: {(df['crawl_status'] == 'FAILED').sum()}")
        print(f"  ì›¹ì‚¬ì´íŠ¸ ì—†ìŒ: {(df['error'] == 'NO_WEBSITE').sum()}")
        
        print(f"\nğŸ“„ ë¬¸ì„œ í†µê³„:")
        print(f"  ì´ ë°©ë¬¸ í˜ì´ì§€: {df['pages_visited'].sum()}")
        print(f"  ì´ ìƒì„± ì²­í¬: {df['chunks_created'].sum()}")
        print(f"  ì´ ì €ì¥ ì²­í¬: {df['chunks_saved'].sum()}")
        print(f"  í‰ê·  ì²­í¬/ì—°êµ¬ì‹¤: {df['chunks_saved'].mean():.1f}")
        
        # DB í†µê³„
        if USE_LOCAL:
            # ë¡œì»¬ ì €ì¥ì†Œ í†µê³„
            stats = self.db.get_stats()
            
            print(f"\nğŸ’¾ ë¡œì»¬ ì €ì¥ì†Œ í†µê³„:")
            print(f"  ì´ ë¬¸ì„œ: {stats['total_docs']}")
            print(f"  í‰ê·  í’ˆì§ˆ: {stats.get('avg_quality_score', 0):.1f}")
            
            if 'section_distribution' in stats:
                print(f"\nğŸ“‚ ì„¹ì…˜ ë¶„í¬:")
                for section, count in stats['section_distribution'].items():
                    print(f"    {section}: {count}")
            
            if 'language_distribution' in stats:
                print(f"\nğŸŒ ì–¸ì–´ ë¶„í¬:")
                for lang, count in stats['language_distribution'].items():
                    print(f"    {lang}: {count}")
        
        else:
            # PostgreSQL í†µê³„ (ì£¼ì„ì²˜ë¦¬ - ë‚˜ì¤‘ì— ë³µì› ê°€ëŠ¥)
            # with VectorDatabase(self.db_config) as db:
            #     stats = db.get_stats()
            #     ... (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
            pass


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ì¸í•˜ëŒ€ ì „ê¸°ì»´í“¨í„°ê³µí•™ê³¼ ì—°êµ¬ì‹¤ í¬ë¡¤ëŸ¬ v2.0")
    print("   - ì²­í‚¹ & ë³¸ë¬¸ ì¶”ì¶œ")
    print("   - í…ìŠ¤íŠ¸ ì •ê·œí™”")
    print("   - ë©€í‹°ë§ê¶ ì„ë² ë”©")
    if USE_LOCAL:
        print("   - ë¡œì»¬ JSON íŒŒì¼ ì €ì¥")
    else:
        print("   - PostgreSQL + pgvector ì €ì¥")
    print()
    
    # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”
    if USE_LOCAL:
        # ë¡œì»¬ ì €ì¥ì†Œ ì‚¬ìš©
        orchestrator = CrawlOrchestrator(
            embedding_model='multilingual-mpnet',
            device='cpu',
            local_data_dir='./crawl_data'
        )
    else:
        # PostgreSQL ì‚¬ìš© (ì£¼ì„ì²˜ë¦¬ - ë‚˜ì¤‘ì— ë³µì› ê°€ëŠ¥)
        # db_config = DatabaseConfig(
        #     host='localhost',
        #     port=5432,
        #     database='labsearch',
        #     user='postgres',
        #     password='postgres'
        # )
        # orchestrator = CrawlOrchestrator(
        #     db_config=db_config,
        #     embedding_model='multilingual-mpnet',
        #     device='cpu'
        # )
        print("âŒ PostgreSQL ëª¨ë“œëŠ” ì£¼ì„ì„ í•´ì œí•˜ê³  ì„¤ì •ì„ ì…ë ¥í•˜ì„¸ìš”")
        return
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    url = "https://inhaece.co.kr/page/labs05"
    df_result = orchestrator.crawl_from_url(url)
    
    # ê²°ê³¼ ì €ì¥
    print("\n" + "="*80)
    print("ê²°ê³¼ ì €ì¥")
    print("="*80)
    
    df_result.to_csv('crawl_results.csv', index=False, encoding='utf-8-sig')
    print("âœ… crawl_results.csv ì €ì¥ ì™„ë£Œ")
    
    # ìš”ì•½ ì¶œë ¥
    orchestrator.print_summary(df_result)
    
    print("\n" + "="*80)
    print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
    print("="*80)
    
    if USE_LOCAL:
        print("\nğŸ’¡ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸:")
        print("   python -c \"from local_storage import LocalVectorStore; from embedding import EmbeddingPipeline; store = LocalVectorStore('./crawl_data'); pipeline = EmbeddingPipeline(); q = pipeline.embed('ì»´í“¨í„° ë¹„ì „'); results = store.search_vector(q.embedding, limit=5); [print(f'{i+1}. [{r.lab_name}] {r.text[:50]}... (ì ìˆ˜: {r.score:.3f})') for i, r in enumerate(results)]\"")
    else:
        print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print("   uvicorn search_api:app --reload")


if __name__ == "__main__":
    main()
