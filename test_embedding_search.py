"""
ì„ë² ë”© & ê²€ìƒ‰ í…ŒìŠ¤íŠ¸

ì‹¤ì œ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©í•˜ê³  ìœ ì‚¬ë„ ê²€ìƒ‰ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import numpy as np
import hashlib
from embedding import EmbeddingPipeline
from local_storage import LocalVectorStore


def test_embedding_and_search():
    """ì„ë² ë”© ìƒì„± ë° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    print("=" * 60)
    print("ì„ë² ë”© & ê²€ìƒ‰ ì¢…í•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # 1. ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    print("\n[1ë‹¨ê³„] ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
    pipeline = EmbeddingPipeline(
        model_name='multilingual-mpnet',  # ì§€ì›ë˜ëŠ” ëª¨ë¸ ì´ë¦„
        device='cpu'
    )
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    # 2. ìƒ˜í”Œ ì—°êµ¬ì‹¤ ë¬¸ì„œë“¤
    print("\n[2ë‹¨ê³„] ìƒ˜í”Œ ë¬¸ì„œ ì¤€ë¹„...")
    
    sample_docs = [
        {
            'lab_name': 'Computer Vision Lab',
            'text': 'ìš°ë¦¬ ì—°êµ¬ì‹¤ì€ ì»´í“¨í„° ë¹„ì „ê³¼ ë”¥ëŸ¬ë‹ì„ ì—°êµ¬í•©ë‹ˆë‹¤. íŠ¹íˆ ê°ì²´ íƒì§€, ì´ë¯¸ì§€ ë¶„í• , 3D ì¬êµ¬ì„± ë“±ì˜ ë¶„ì•¼ì— ì§‘ì¤‘í•˜ê³  ìˆìŠµë‹ˆë‹¤.',
            'section': 'research'
        },
        {
            'lab_name': 'NLP Lab',
            'text': 'ìì—°ì–´ ì²˜ë¦¬ì™€ ëŒ€í™” ì‹œìŠ¤í…œì„ ì—°êµ¬í•©ë‹ˆë‹¤. BERT, GPT ë“±ì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ í™œìš©í•œ í•œêµ­ì–´ ì²˜ë¦¬ ê¸°ìˆ ì„ ê°œë°œí•˜ê³  ìˆìŠµë‹ˆë‹¤.',
            'section': 'research'
        },
        {
            'lab_name': 'Robotics Lab',
            'text': 'ë¡œë´‡ ì œì–´ì™€ ììœ¨ ì£¼í–‰ ì‹œìŠ¤í…œì„ ì—°êµ¬í•©ë‹ˆë‹¤. ì„¼ì„œ ìœµí•©, ê²½ë¡œ ê³„íš, ê°•í™”í•™ìŠµ ê¸°ë°˜ ë¡œë´‡ ì œì–´ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.',
            'section': 'research'
        },
        {
            'lab_name': 'Computer Vision Lab',
            'text': 'CVPR 2024ì— 2í¸, ICCV 2023ì— 1í¸ì˜ ë…¼ë¬¸ì´ ì±„íƒë˜ì—ˆìŠµë‹ˆë‹¤. ì£¼ìš” ì£¼ì œëŠ” Transformer ê¸°ë°˜ ê°ì²´ íƒì§€ì…ë‹ˆë‹¤.',
            'section': 'publication'
        },
        {
            'lab_name': 'NLP Lab',
            'text': 'í•™ë¶€ ì—°êµ¬ìƒì„ ëª¨ì§‘í•©ë‹ˆë‹¤. ì£¼ë‹¹ 10ì‹œê°„ ì´ìƒ ì°¸ì—¬ ê°€ëŠ¥í•œ í•™ìƒì„ ì„ ë°œí•©ë‹ˆë‹¤. ê¸°ê³„í•™ìŠµ ê¸°ì´ˆ ì§€ì‹ì´ í•„ìš”í•©ë‹ˆë‹¤.',
            'section': 'join'
        },
    ]
    
    print(f"âœ… {len(sample_docs)}ê°œ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ")
    
    # 3. ì„ë² ë”© ìƒì„±
    print("\n[3ë‹¨ê³„] ì„ë² ë”© ìƒì„± ì¤‘...")
    
    embeddings = []
    for i, doc in enumerate(sample_docs):
        result = pipeline.embed(doc['text'])
        embeddings.append(result.embedding)
        print(f"  ë¬¸ì„œ {i+1}/{len(sample_docs)}: {doc['lab_name']} - {doc['section']}")
        print(f"    ë²¡í„° ì°¨ì›: {len(result.embedding)}")
        print(f"    ì •ê·œí™” ì—¬ë¶€: {result.normalized}")
    
    print(f"âœ… ì´ {len(embeddings)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    
    # 4. ë¡œì»¬ ì €ì¥ì†Œì— ì €ì¥
    print("\n[4ë‹¨ê³„] ë¡œì»¬ ì €ì¥ì†Œì— ì €ì¥...")
    
    store = LocalVectorStore(data_dir='./test_embedding_data')
    
    # ì—°êµ¬ì‹¤ ID ë§¤í•‘
    lab_ids = {}
    
    # ì—°êµ¬ì‹¤ ìƒì„±
    for lab_name in set(doc['lab_name'] for doc in sample_docs):
        lab_id = store.insert_lab({
            'kor_name': lab_name,
            'eng_name': lab_name,
            'homepage': f"https://example.com/{lab_name.lower().replace(' ', '_')}",
            'description': f"{lab_name} ì—°êµ¬ì‹¤"
        })
        lab_ids[lab_name] = lab_id
    
    # ë¬¸ì„œ ì‚½ì…
    for doc, emb in zip(sample_docs, embeddings):
        lab_id = lab_ids[doc['lab_name']]
        
        store.insert_document(
            lab_id=lab_id,
            doc_data={
                'text': doc['text'],
                'embedding': emb.tolist(),
                'section': doc['section'],
                'char_count': len(doc['text']),
                'md5': hashlib.md5(doc['text'].encode()).hexdigest()
            }
        )
    
    stats = store.get_stats()
    print(f"âœ… ì €ì¥ ì™„ë£Œ: ì—°êµ¬ì‹¤ {stats['total_labs']}ê°œ, ë¬¸ì„œ {stats['total_docs']}ê°œ")
    
    # 5. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n[5ë‹¨ê³„] ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
    print("=" * 60)
    
    queries = [
        "ë”¥ëŸ¬ë‹ê³¼ ì»´í“¨í„° ë¹„ì „ìœ¼ë¡œ ì´ë¯¸ì§€ ë¶„ì„",
        "ìì—°ì–´ ì²˜ë¦¬ í•œêµ­ì–´ ëª¨ë¸",
        "ë¡œë´‡ ììœ¨ì£¼í–‰ ê°•í™”í•™ìŠµ",
        "CVPR ë…¼ë¬¸ ê°ì²´ íƒì§€",
        "í•™ë¶€ìƒ ì—°êµ¬ì‹¤ ëª¨ì§‘"
    ]
    
    for query in queries:
        print(f"\nğŸ” ì¿¼ë¦¬: '{query}'")
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_result = pipeline.embed(query)
        query_emb = query_result.embedding
        
        # ê²€ìƒ‰
        results = store.search_vector(
            query_embedding=query_emb,
            limit=3
        )
        
        print(f"   ê²°ê³¼ {len(results)}ê°œ:")
        
        for i, result in enumerate(results, 1):
            print(f"\n   {i}. [{result.lab_name}] (ìœ ì‚¬ë„: {result.score:.3f})")
            print(f"      ì„¹ì…˜: {result.section}")
            print(f"      ë‚´ìš©: {result.text[:80]}...")
    
    # 6. í†µê³„ ì¶œë ¥
    print("\n" + "=" * 60)
    print("[í†µê³„]")
    print("=" * 60)
    
    stats = store.get_stats()
    print(f"ì´ ì—°êµ¬ì‹¤: {stats['total_labs']}")
    print(f"ì´ ë¬¸ì„œ: {stats['total_docs']}")
    print(f"í‰ê·  ë¬¸ì„œ ê¸¸ì´: {stats.get('avg_char_count', 0):.1f}ì")
    
    if 'section_distribution' in stats:
        print("\nì„¹ì…˜ë³„ ë¶„í¬:")
        for section, count in stats['section_distribution'].items():
            print(f"  {section}: {count}ê°œ")
    
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    test_embedding_and_search()
