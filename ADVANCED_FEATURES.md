# ê³ ê¸‰ ê¸°ëŠ¥ êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ êµ¬í˜„ëœ ê¸°ëŠ¥ ëª©ë¡

### âœ… 1. í’ˆì§ˆ/ê°€ë“œë ˆì¼ ì‹œìŠ¤í…œ

**íŒŒì¼:** `quality_guard.py`

#### 1.1 í’ˆì§ˆ ì ìˆ˜ (QualityScorer)
- **quality_score**: 0.0-1.0 ì ìˆ˜ ê³„ì‚°
  - ì„¹ì…˜ ì¼ì¹˜ë„ (30%)
  - ê¸¸ì´ ì ì ˆì„± (25%) 
  - ì–¸ì–´ ì¼ê´€ì„± (25%)
  - ì¤‘ë³µ ì—¬ë¶€ (20%)
- **needs_review**: 0.5 ë¯¸ë§Œì´ë©´ ê²€ìˆ˜ ëŒ€ìƒìœ¼ë¡œ í‘œì‹œ
- **review_reason**: ë‚®ì€ ì ìˆ˜ì˜ ì´ìœ  ìë™ ìƒì„±

#### 1.2 PII/ë¹„ê³µê°œ ì°¨ë‹¨ (GuardRail)
- í¬í„¸/ë¡œê·¸ì¸/ê°œì¸ì •ë³´ í˜ì´ì§€ ê°ì§€
- URL íŒ¨í„´ ë§¤ì¹­: `/login`, `/admin`, `/portal` ë“±
- í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ê°ì§€: 'ë¹„ë°€ë²ˆí˜¸', 'password' ë“±
- HTML í¼ ë¶„ì„: password/email ì…ë ¥ í•„ë“œ ê°ì§€

```python
from quality_guard import GuardRail, QualityScorer

# ê°€ë“œë ˆì¼ ì‚¬ìš©
guard = GuardRail()
should_exclude, reason = guard.should_exclude_url(url)
has_pii, keywords = guard.detect_pii_in_text(text)

# í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
scorer = QualityScorer()
report = scorer.calculate_quality(chunk, all_chunks)
if report.needs_review:
    print(f"ê²€ìˆ˜ í•„ìš”: {report.reason}")
```

---

### âœ… 2. í¬ë¡¤ë§ ë§¤ë‹ˆì € (CrawlManager)

**íŒŒì¼:** `crawl_manager.py`

#### 2.1 robots.txt ì¤€ìˆ˜
- ë„ë©”ì¸ë³„ robots.txt ìë™ ë¡œë“œ
- User-Agentë³„ í—ˆìš© ì—¬ë¶€ í™•ì¸
- ìºì‹±ìœ¼ë¡œ ë°˜ë³µ ìš”ì²­ ë°©ì§€

#### 2.2 ì†ë„ ì œì–´
- `delay` íŒŒë¼ë¯¸í„°ë¡œ ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ ì„¤ì •
- ê¸°ë³¸ê°’: 1.0ì´ˆ (ê¶Œì¥: 0.5-1.0 req/sec)
- ë§ˆì§€ë§‰ ìš”ì²­ ì‹œê°„ ì¶”ì 

#### 2.3 ì‹¤íŒ¨ ì¬ì‹œë„ (ì§€ìˆ˜ ë°±ì˜¤í”„)
- ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜: 3íšŒ (ì„¤ì • ê°€ëŠ¥)
- ëŒ€ê¸° ì‹œê°„: 1ì´ˆ â†’ 2ì´ˆ â†’ 4ì´ˆ â†’ 8ì´ˆ...
- íƒ€ì„ì•„ì›ƒ, ì—°ê²° ì˜¤ë¥˜ ìë™ ì²˜ë¦¬

#### 2.4 ETag/Last-Modified ìºì‹±
- HTTP ìºì‹œ í—¤ë” ìë™ ì €ì¥
- 304 Not Modified ì‘ë‹µ ì²˜ë¦¬
- ìºì‹œ ìœ íš¨ ê¸°ê°„: 7ì¼
- ë””ìŠ¤í¬ ìºì‹œ (`./crawl_cache/`)

#### 2.5 í†µê³„ ë° ë¡œê¹…
- ì´ ìš”ì²­ ìˆ˜, ì„±ê³µ/ì‹¤íŒ¨ ìˆ˜
- ìºì‹œ ì‚¬ìš© íšŸìˆ˜
- ì¬ì‹œë„ íšŸìˆ˜
- ì„±ê³µë¥  ê³„ì‚°

```python
from crawl_manager import CrawlManager

# ê¸°ë³¸ ì‚¬ìš©
manager = CrawlManager(
    delay=1.0,              # 1ì´ˆ ë”œë ˆì´
    max_retries=3,          # ìµœëŒ€ 3íšŒ ì¬ì‹œë„
    user_agent="YourBot/1.0"
)

result = manager.fetch_url(url)
if result.success:
    html = result.html
    print(f"ìºì‹œ ì‚¬ìš©: {result.cached}")

# í†µê³„ í™•ì¸
manager.print_stats()
```

---

### âœ… 3. ì—…ë°ì´íŠ¸ ì „ëµ

**íŒŒì¼:** `schema_enhanced.sql`

#### 3.1 ì¬í¬ë¡¤ ì£¼ê¸° ê´€ë¦¬
- `last_crawled_at` í•„ë“œë¡œ ë§ˆì§€ë§‰ í¬ë¡¤ë§ ì‹œê°„ ì¶”ì 
- 2-4ì£¼ ì£¼ê¸° ê¶Œì¥
- ETag/Last-Modifiedë¡œ ë³€ê²½ ê°ì§€

#### 3.2 ì†Œí”„íŠ¸ ì‚­ì œ
- `is_active` í•„ë“œ (ê¸°ë³¸ê°’: TRUE)
- ì›ë¬¸ì—ì„œ ì‚¬ë¼ì§„ ë¬¸ì„œ â†’ `is_active=false`
- íˆìŠ¤í† ë¦¬ ë³´ì¡´ (ì™„ì „ ì‚­ì œ ì•ˆ í•¨)
- `soft_delete_document(doc_id)` í•¨ìˆ˜ ì œê³µ

```sql
-- ë¬¸ì„œ ì†Œí”„íŠ¸ ì‚­ì œ
SELECT soft_delete_document(123);

-- í™œì„± ë¬¸ì„œë§Œ ì¡°íšŒ
SELECT * FROM documents WHERE is_active = TRUE;

-- ì‚­ì œëœ ë¬¸ì„œ ë³µêµ¬
UPDATE documents SET is_active = TRUE WHERE id = 123;
```

#### 3.3 ê°ì‚¬ ê°€ëŠ¥ì„± (crawl_log í…Œì´ë¸”)
- ëª¨ë“  í¬ë¡¤ë§ ìš”ì²­ ë¡œê·¸ ì €ì¥
- ìƒíƒœ ì½”ë“œ, ì‘ë‹µ ì‹œê°„, ì—ëŸ¬ ì‚¬ìœ 
- ìƒì„±/ì œì™¸ëœ ì²­í¬ ìˆ˜
- ìºì‹œ ì‚¬ìš© ì—¬ë¶€

**crawl_log í…Œì´ë¸” ì»¬ëŸ¼:**
- `url`, `status_code`, `success`
- `response_time_ms`, `response_size`
- `error_message`, `error_type`
- `chunks_created`, `chunks_excluded`
- `used_cache`, `etag`, `last_modified`

#### 3.4 ì—…ë°ì´íŠ¸ ì´ë ¥ ì¶”ì 
- `update_history` í…Œì´ë¸”
- ë³€ê²½ íƒ€ì…: added, modified, deleted
- ì´ì „ ê°’ vs ìƒˆ ê°’ ì €ì¥
- ë³€ê²½ ê°ì§€ ì‹œê°„, í¬ë¡¤ ë¡œê·¸ ì—°ê²°

```sql
-- ìµœê·¼ ë³€ê²½ ì´ë ¥ ì¡°íšŒ
SELECT * FROM update_history 
WHERE lab_id = 1 
ORDER BY detected_at DESC 
LIMIT 10;
```

---

### âœ… 4. PDF/í‘œ/ì´ë¯¸ì§€ ëŒ€ì‘

**íŒŒì¼:** `advanced_extractors.py`

#### 4.1 PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (PDFExtractor)
- PyPDF2 ë˜ëŠ” pdfplumber ì§€ì›
- í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ì œëª©, ì €ì, í˜ì´ì§€ ìˆ˜)
- `source_type='pdf'`ë¡œ í‘œê¸°

```python
from advanced_extractors import PDFExtractor

extractor = PDFExtractor(backend='pypdf2')
text = extractor.extract_text("paper.pdf")
metadata = extractor.extract_metadata("paper.pdf")

print(f"ì œëª©: {metadata['title']}")
print(f"í˜ì´ì§€: {metadata['pages']}")
```

#### 4.2 í‘œ êµ¬ì¡° ë³´ì¡´ (TableExtractor)
- HTML í‘œ ìë™ ì¶”ì¶œ
- í—¤ë” ì¸ì‹ (venue, year, title, author)
- ì»¬ëŸ¼ ë§¤í•‘ ìë™ ìƒì„±
- **lab_tag** ìë™ ìƒì„± (venue + year)
- í…ìŠ¤íŠ¸/ë”•ì…”ë„ˆë¦¬ ë³€í™˜ ì§€ì›

```python
from advanced_extractors import TableExtractor

extractor = TableExtractor()
tables = extractor.extract_tables(html)

for table in tables:
    # í…ìŠ¤íŠ¸ í˜•ì‹
    print(table.to_text())
    
    # ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ (JSON ì¹œí™”ì )
    for row_dict in table.to_dict_list():
        print(row_dict)
    
    # ë©”íƒ€ë°ì´í„° í™•ì¸
    if 'lab_tags' in table.metadata:
        print(f"ë…¼ë¬¸ íƒœê·¸: {table.metadata['lab_tags']}")
```

#### 4.3 ì´ë¯¸ì§€ OCR (ImageOCR) - ì„ íƒì 
- pytesseract + Pillow ì‚¬ìš©
- í•œê¸€/ì˜ë¬¸ ë™ì‹œ ì¸ì‹
- **ë¹„ê¶Œì¥Â·í›„ìˆœìœ„** (ì •í™•ë„ ë‚®ìŒ)

```python
from advanced_extractors import ImageOCR

ocr = ImageOCR()
text = ocr.extract_text("image.png", lang='kor+eng')
```

**ì˜ì¡´ì„± ì„¤ì¹˜:**
```bash
pip install PyPDF2 pdfplumber pytesseract pillow
```

---

### âœ… 5. ê²€ìƒ‰/ì¶”ì²œ ë©”íƒ€ë°ì´í„°

**íŒŒì¼:** `schema_enhanced.sql`

#### 5.1 Signals (ì¬ë­í‚¹ ê°€ì )
- `recent_papers_count`: ìµœê·¼ 3ë…„ ë…¼ë¬¸ ìˆ˜
- `has_awards`: ìˆ˜ìƒ ì´ë ¥ ì—¬ë¶€
- `equipment_gpu`: GPU ì¥ë¹„ ìˆ˜
- `equipment_robot`: ë¡œë´‡ ì¥ë¹„ ì—¬ë¶€

```sql
-- GPU ë§ì€ ì—°êµ¬ì‹¤ ìš°ì„ 
SELECT * FROM labs 
WHERE is_active = TRUE 
ORDER BY equipment_gpu DESC, recent_papers_count DESC;
```

#### 5.2 Constraints (ëª¨ì§‘ ì¡°ê±´)
- `min_hours`: ìµœì†Œ ì°¸ì—¬ ì‹œê°„ (ì‹œê°„/ì£¼)
- `weekend_ok`: ì£¼ë§ ê°€ëŠ¥ ì—¬ë¶€
- `join_type`: í•™ë¶€ì—°êµ¬ìƒ/ëŒ€í•™ì›/ì¸í„´

```sql
-- ì£¼ë§ ê°€ëŠ¥í•œ ì—°êµ¬ì‹¤ ê²€ìƒ‰
SELECT * FROM labs 
WHERE weekend_ok = TRUE AND is_active = TRUE;
```

#### 5.3 Provenance (ì¶”ì²œ ì´ìœ )
- `matched_snippet`: ë§¤ì¹­ëœ ë¬¸ì¥ ìºì‹œ
- ê²€ìƒ‰ ê²°ê³¼ì— "ì™œ ì´ ì—°êµ¬ì‹¤ì´ ì¶”ì²œë˜ì—ˆëŠ”ì§€" í‘œì‹œìš©

```sql
UPDATE documents 
SET matched_snippet = 'ìš°ë¦¬ ì—°êµ¬ì‹¤ì€ ì»´í“¨í„° ë¹„ì „ì„ ì—°êµ¬í•©ë‹ˆë‹¤.'
WHERE id = 123;
```

---

## ğŸ”§ í†µí•© ì‚¬ìš© ì˜ˆì‹œ

### ì „ì²´ íŒŒì´í”„ë¼ì¸ì— í†µí•©

```python
# main_pipeline.pyì— ì¶”ê°€

from quality_guard import GuardRail, QualityScorer
from crawl_manager import CrawlManager
from advanced_extractors import PDFExtractor, TableExtractor

class EnhancedCrawlOrchestrator:
    def __init__(self):
        # ê¸°ì¡´ ì»´í¬ë„ŒíŠ¸
        self.extractor = ContentExtractor()
        self.chunker = TextChunker()
        self.normalizer = TextNormalizer()
        self.embedder = EmbeddingPipeline()
        
        # ìƒˆë¡œìš´ ì»´í¬ë„ŒíŠ¸
        self.guard = GuardRail()
        self.scorer = QualityScorer()
        self.crawl_manager = CrawlManager(delay=1.0, max_retries=3)
        self.pdf_extractor = PDFExtractor()
        self.table_extractor = TableExtractor()
    
    def crawl_with_quality_check(self, url: str):
        # 1. URL ì°¨ë‹¨ í™•ì¸
        should_exclude, reason = self.guard.should_exclude_url(url)
        if should_exclude:
            print(f"ì°¨ë‹¨: {reason}")
            return None
        
        # 2. í¬ë¡¤ë§ (ì†ë„ ì œí•œ, ì¬ì‹œë„ í¬í•¨)
        result = self.crawl_manager.fetch_url(url)
        if not result.success:
            print(f"ì‹¤íŒ¨: {result.error}")
            return None
        
        html = result.html
        
        # 3. PII ê°ì§€
        has_pii, keywords = self.guard.detect_pii_in_html(html)
        if has_pii:
            print(f"PII ë°œê²¬: {keywords}")
            return None
        
        # 4. í‘œ ì¶”ì¶œ (ìˆìœ¼ë©´)
        tables = self.table_extractor.extract_tables(html)
        for table in tables:
            if 'lab_tags' in table.metadata:
                print(f"ë…¼ë¬¸ íƒœê·¸: {table.metadata['lab_tags']}")
        
        # 5. ì½˜í…ì¸  ì¶”ì¶œ ë° ì²­í‚¹
        text = self.extractor.clean_html(html, url)
        chunks = self.chunker.chunk_text(text, source_url=url)
        
        # 6. í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_checked_chunks = []
        for chunk in chunks:
            report = self.scorer.calculate_quality(chunk, chunks)
            
            chunk.quality_score = report.overall_score
            chunk.needs_review = report.needs_review
            
            if report.needs_review:
                print(f"ê²€ìˆ˜ í•„ìš”: {report.reason}")
            
            quality_checked_chunks.append(chunk)
        
        # 7. ë‚˜ë¨¸ì§€ ì²˜ë¦¬ (ì •ê·œí™”, ì„ë² ë”©, ì €ì¥)
        # ...
        
        return quality_checked_chunks
```

---

## ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ í™œìš©

### í’ˆì§ˆ ì ìˆ˜ë¡œ ê²€ìˆ˜ ëŒ€ìƒ ì°¾ê¸°

```sql
-- ê²€ìˆ˜ê°€ í•„ìš”í•œ ë¬¸ì„œ
SELECT d.id, d.text, d.quality_score, d.review_reason
FROM documents d
WHERE d.needs_review = TRUE
ORDER BY d.quality_score ASC
LIMIT 20;
```

### í¬ë¡¤ë§ í†µê³„ ë³´ê¸°

```sql
-- ì¼ë³„ í¬ë¡¤ë§ í†µê³„
SELECT * FROM crawl_statistics 
ORDER BY crawl_date DESC 
LIMIT 7;

-- ìµœê·¼ ì—ëŸ¬ ë¡œê·¸
SELECT url, error_message, error_type, request_time
FROM crawl_log
WHERE success = FALSE
ORDER BY request_time DESC
LIMIT 10;
```

### í™œì„± ì—°êµ¬ì‹¤ ìš”ì•½

```sql
-- ì—°êµ¬ì‹¤ë³„ ë¬¸ì„œ í’ˆì§ˆ ìš”ì•½
SELECT * FROM active_labs_summary
ORDER BY avg_quality DESC;
```

---

## ğŸ“¦ ì˜ì¡´ì„± ì„¤ì¹˜

ëª¨ë“  ê³ ê¸‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´:

```bash
# ê¸°ë³¸ (í•„ìˆ˜)
pip install beautifulsoup4 requests

# PDF ì§€ì›
pip install PyPDF2
# ë˜ëŠ” (ë” ì •í™•)
pip install pdfplumber

# OCR ì§€ì› (ì„ íƒ)
pip install pytesseract pillow
# + Tesseract OCR ì—”ì§„ ì„¤ì¹˜ (ì‹œìŠ¤í…œ ë ˆë²¨)
```

---

## âš™ï¸ ì„¤ì • ì˜ˆì‹œ

### config.py (ì„¤ì • íŒŒì¼)

```python
# í¬ë¡¤ë§ ì„¤ì •
CRAWL_DELAY = 1.0  # ì´ˆ
MAX_RETRIES = 3
REQUEST_TIMEOUT = 10

# í’ˆì§ˆ ì ìˆ˜ ì„ê³„ê°’
QUALITY_THRESHOLD = 0.5  # ì´í•˜ë©´ ê²€ìˆ˜ í•„ìš”

# ìºì‹œ ì„¤ì •
CACHE_DIR = './crawl_cache'
CACHE_EXPIRY_DAYS = 7

# ì¬í¬ë¡¤ ì£¼ê¸°
RECRAWL_INTERVAL_DAYS = 21  # 3ì£¼

# User-Agent
USER_AGENT = "INHA-LabSearch-Bot/1.0 (Educational; Contact: your-email@inha.ac.kr)"
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **í’ˆì§ˆ ëª¨ë‹ˆí„°ë§**: `quality_guard.py` í…ŒìŠ¤íŠ¸
2. **í¬ë¡¤ë§ ì†ë„ ì¡°ì ˆ**: `crawl_manager.py`ë¡œ ì˜ˆì˜ë°”ë¥¸ í¬ë¡¤ë§
3. **ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜**: `schema_enhanced.sql` ì ìš©
4. **PDF ì§€ì› ì¶”ê°€**: ë…¼ë¬¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œ í™œìš©
5. **í‘œ ë°ì´í„° í™œìš©**: ë…¼ë¬¸ ëª©ë¡ì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì €ì¥

---

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- `quality_guard.py` - í’ˆì§ˆ ì ìˆ˜ ë° PII ê°ì§€
- `crawl_manager.py` - í¬ë¡¤ë§ ë§¤ë‹ˆì € (ì†ë„, ì¬ì‹œë„, ìºì‹±)
- `advanced_extractors.py` - PDF, í‘œ, OCR ì²˜ë¦¬
- `schema_enhanced.sql` - í–¥ìƒëœ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ

ëª¨ë“  íŒŒì¼ì—ëŠ” ì‚¬ìš© ì˜ˆì‹œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤!
